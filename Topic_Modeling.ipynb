{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUROKyia1Xj4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2df1bb7b-4d0b-4a44-8c0a-db096e2fb65e"
      },
      "source": [
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import lemmatize, simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO5s2IQcmTfl"
      },
      "source": [
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees\",\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMyvavJ1mbHt"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHutQDQTmWJs"
      },
      "source": [
        "# NLTK Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HpLqf1j1gP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "06443043-e8e0-4ef8-a050-1a155dbe809a"
      },
      "source": [
        "# Import dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "df = df.loc[df.target_names.isin(['soc.religion.christian', 'rec.sport.hockey', 'talk.politics.mideast', 'rec.motorcycles']) , :]\n",
        "print(df.shape)  #> (2361, 3)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2361, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
              "      <td>8</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>From: leunggm@odin.control.utoronto.ca (Gary L...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From: jonh@david.wheaton.edu (Jonathan Hayward...</td>\n",
              "      <td>15</td>\n",
              "      <td>soc.religion.christian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>From: ayr1@cunixa.cc.columbia.edu (Amir Y Rose...</td>\n",
              "      <td>17</td>\n",
              "      <td>talk.politics.mideast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>From: dchhabra@stpl.ists.ca (Deepak Chhabra)\\n...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              content  ...            target_names\n",
              "10  From: irwin@cmptrc.lonestar.org (Irwin Arnstei...  ...         rec.motorcycles\n",
              "21  From: leunggm@odin.control.utoronto.ca (Gary L...  ...        rec.sport.hockey\n",
              "28  From: jonh@david.wheaton.edu (Jonathan Hayward...  ...  soc.religion.christian\n",
              "33  From: ayr1@cunixa.cc.columbia.edu (Amir Y Rose...  ...   talk.politics.mideast\n",
              "35  From: dchhabra@stpl.ists.ca (Deepak Chhabra)\\n...  ...        rec.sport.hockey\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQuTIzn1k9R"
      },
      "source": [
        "import re\n",
        "#Tokenize and clean\n",
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OBarb0c1pBE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "43cb759a-bf3c-4f42-8d33-a4914666fbdd"
      },
      "source": [
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['from', 'irwin', 'arnstein', 'subject', 're', 'recommendation', 'on', 'duc', 'summary', 'whats', 'it', 'worth', 'distribution', 'usa', 'expires', 'sat', 'may', 'gmt', 'organization', 'computrac', 'inc', 'richardson', 'tx', 'keywords', 'ducati', 'gts', 'how', 'much', 'lines', 'have', 'line', 'on', 'ducati', 'gts', 'model', 'with', 'on', 'the', 'clock', 'runs', 'very', 'well', 'paint', 'is', 'the', 'bronze', 'brown', 'orange', 'faded', 'out', 'leaks', 'bit', 'of', 'oil', 'and', 'pops', 'out', 'of', 'st', 'with', 'hard', 'accel', 'the', 'shop', 'will', 'fix', 'trans', 'and', 'oil', 'leak', 'they', 'sold', 'the', 'bike', 'to', 'the', 'and', 'only', 'owner', 'they', 'want', 'and', 'am', 'thinking', 'more', 'like', 'any', 'opinions', 'out', 'there', 'please', 'email', 'me', 'thanks', 'it', 'would', 'be', 'nice', 'stable', 'mate', 'to', 'the', 'beemer', 'then', 'ill', 'get', 'jap', 'bike', 'and', 'call', 'myself', 'axis', 'motors', 'tuba', 'irwin', 'honk', 'therefore', 'am', 'computrac', 'richardson', 'tx', 'dod']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fZkXnmk1uX_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "96a62409-db46-4b28-a1e3-39cc83f102ef"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClKfg_MK1vRT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "026f8473-316e-44f9-9be7-b255eb3f7d11"
      },
      "source": [
        "!python3 -m spacy download en  # run in terminal once\n",
        "# or do\n",
        "# !conda install -c conda-forge spacy-model-en_core_web_md \n",
        "# and use nlp=spacy.load('en_core_web_sm') instead in below function.\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts = [bigram_mod[doc] for doc in texts]\n",
        "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    # remove stopwords once more after lemmatization\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "data_ready = process_words(data_words)  # processed Text Data!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GYZvAzp1zlb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "ac4af60d-5fb0-4938-9975-90507e493425"
      },
      "source": [
        "from pprint import pprint\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=4, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.012*\"state\" + 0.012*\"israeli\" + 0.011*\"people\" + 0.011*\"kill\" + '\n",
            "  '0.009*\"attack\" + 0.009*\"government\" + 0.008*\"war\" + 0.007*\"turkish\" + '\n",
            "  '0.006*\"soldier\" + 0.006*\"greek\"'),\n",
            " (1,\n",
            "  '0.020*\"game\" + 0.018*\"bike\" + 0.017*\"write\" + 0.012*\"article\" + '\n",
            "  '0.009*\"rider\" + 0.008*\"list\" + 0.008*\"ride\" + 0.007*\"score\" + '\n",
            "  '0.006*\"motorcycle\" + 0.006*\"helmet\"'),\n",
            " (2,\n",
            "  '0.017*\"team\" + 0.015*\"year\" + 0.012*\"time\" + 0.011*\"write\" + 0.009*\"well\" + '\n",
            "  '0.009*\"first\" + 0.009*\"play\" + 0.008*\"look\" + 0.008*\"help\" + 0.008*\"name\"'),\n",
            " (3,\n",
            "  '0.014*\"people\" + 0.012*\"write\" + 0.010*\"believe\" + 0.008*\"reason\" + '\n",
            "  '0.007*\"evidence\" + 0.006*\"question\" + 0.006*\"thing\" + 0.006*\"article\" + '\n",
            "  '0.006*\"claim\" + 0.005*\"faith\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1qr7ElDNbZ"
      },
      "source": [
        "### What is the Dominant topic and its percentage contribution in each document\n",
        "\n",
        "#####In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ixv4L9k19dU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "5f03f827-ac7b-4345-d7e6-ea71fa6c778d"
      },
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document_No</th>\n",
              "      <th>Dominant_Topic</th>\n",
              "      <th>Topic_Perc_Contrib</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8037</td>\n",
              "      <td>game, bike, write, article, rider, list, ride,...</td>\n",
              "      <td>[summary, worth, expire, keyword, ducati, gts,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.8796</td>\n",
              "      <td>team, year, time, write, well, first, play, lo...</td>\n",
              "      <td>[group, article, write, course, time, prior, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.7593</td>\n",
              "      <td>people, write, believe, reason, evidence, ques...</td>\n",
              "      <td>[write, new, group, maybe, cover, already, cur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5978</td>\n",
              "      <td>state, israeli, people, kill, attack, governme...</td>\n",
              "      <td>[article, write, article, write, article, writ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.8912</td>\n",
              "      <td>team, year, time, write, well, first, play, lo...</td>\n",
              "      <td>[goalie_mask, organization, ist, article, writ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6098</td>\n",
              "      <td>game, bike, write, article, rider, list, ride,...</td>\n",
              "      <td>[bmw_moa_member, read, internet, access, syste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.5122</td>\n",
              "      <td>team, year, time, write, well, first, play, lo...</td>\n",
              "      <td>[require, organization, article, write, articl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4970</td>\n",
              "      <td>game, bike, write, article, rider, list, ride,...</td>\n",
              "      <td>[looking, movie, bike, university, latech, sum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.8834</td>\n",
              "      <td>people, write, believe, reason, evidence, ques...</td>\n",
              "      <td>[organization, university, follow, thread, tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6453</td>\n",
              "      <td>state, israeli, people, kill, attack, governme...</td>\n",
              "      <td>[day, night, round, male, inhabitant, article,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document_No  ...                                               Text\n",
              "0            0  ...  [summary, worth, expire, keyword, ducati, gts,...\n",
              "1            1  ...  [group, article, write, course, time, prior, t...\n",
              "2            2  ...  [write, new, group, maybe, cover, already, cur...\n",
              "3            3  ...  [article, write, article, write, article, writ...\n",
              "4            4  ...  [goalie_mask, organization, ist, article, writ...\n",
              "5            5  ...  [bmw_moa_member, read, internet, access, syste...\n",
              "6            6  ...  [require, organization, article, write, articl...\n",
              "7            7  ...  [looking, movie, bike, university, latech, sum...\n",
              "8            8  ...  [organization, university, follow, thread, tal...\n",
              "9            9  ...  [day, night, round, male, inhabitant, article,...\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMVCzzaP2AS9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "09c4f86a-33c2-44db-8f8c-8fb650c55ba4"
      },
      "source": [
        "# Display setting to show more characters in column\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index    \n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic_Num</th>\n",
              "      <th>Topic_Perc_Contrib</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Representative Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9667</td>\n",
              "      <td>state, israeli, people, kill, attack, government, war, turkish, soldier, greek</td>\n",
              "      <td>[armenian, genocide, people, article, write, ranada, ermeniler, boyle, icin, bulunan, oldurmuler...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9125</td>\n",
              "      <td>game, bike, write, article, rider, list, ride, score, motorcycle, helmet</td>\n",
              "      <td>[hockey, draft, price, list, price, list, week, buy, team, player, pit, det, tor, pit, pit, tor,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.9783</td>\n",
              "      <td>team, year, time, write, well, first, play, look, help, name</td>\n",
              "      <td>[article, cire, write, write, write, captain, ever, traded_resigne, stripe, title, season, appre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.9871</td>\n",
              "      <td>people, write, believe, reason, evidence, question, thing, article, claim, faith</td>\n",
              "      <td>[article, write, affirm, absolute, scripture, believe, truth, reveal, truth, word, therefore, ho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Topic_Num  ...                                                                                  Representative Text\n",
              "0        0.0  ...  [armenian, genocide, people, article, write, ranada, ermeniler, boyle, icin, bulunan, oldurmuler...\n",
              "1        1.0  ...  [hockey, draft, price, list, price, list, week, buy, team, player, pit, det, tor, pit, pit, tor,...\n",
              "2        2.0  ...  [article, cire, write, write, write, captain, ever, traded_resigne, stripe, title, season, appre...\n",
              "3        3.0  ...  [article, write, affirm, absolute, scripture, believe, truth, reveal, truth, word, therefore, ho...\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GxMtL1x2WmI"
      },
      "source": [
        "#Frequency Distribution of Word Counts in Documents\n",
        "#When working with a large number of documents, you want to know how big the documents are as a whole and by topic. Let’s plot the document word counts distribution.\n",
        "\n",
        "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
        "# Plot\n",
        "plt.figure(figsize=(16,7), dpi=160)\n",
        "plt.hist(doc_lens, bins = 1000, color='navy')\n",
        "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
        "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
        "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
        "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
        "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
        "\n",
        "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
        "plt.tick_params(size=16)\n",
        "plt.xticks(np.linspace(0,1000,9))\n",
        "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.colors as mcolors\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):    \n",
        "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
        "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
        "    ax.hist(doc_lens, bins = 1000, color=cols[i])\n",
        "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
        "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
        "    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
        "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
        "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=0.90)\n",
        "plt.xticks(np.linspace(0,1000,9))\n",
        "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}